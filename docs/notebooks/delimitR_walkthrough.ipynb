{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delimitpy import parse_input\n",
    "from delimitpy import process_empirical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the configuration file\n",
    "\n",
    "We will use the ModelConfigParser class to parse the information from the configuration. A full description of the configuration file, along with an example, is available [here](https://delimitpy.readthedocs.io/en/latest/usage/parsinginput.html). This file provides delimitpy with the information it needs to conduct a species delimitation analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_parser = parse_input.ModelConfigParser(\"../../examples/test1/config.txt\")\n",
    "config_values = config_parser.parse_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process empirical data\n",
    "\n",
    "Next, we will process our empirical data. The format for the empirical data is described in detail [here](https://delimitpy.readthedocs.io/en/latest/usage/parsinginput.html). \n",
    "\n",
    "There are three major steps:\n",
    "1) Read data from fasta files and convert to a numpy array.\n",
    "2) Choose values for down-projecting the Site Frequency Spectrum.\n",
    "3) Build SFS with down-projection.\n",
    "4) Determine the average number of SNPs used to construct the SFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read data into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input data has 30 individuals and 1038 SNPs.\n"
     ]
    }
   ],
   "source": [
    "data_processor = process_empirical.DataProcessor(config=config_values)\n",
    "empirical_array = data_processor.fasta_to_numpy()\n",
    "print(f\"Our input data has {empirical_array.shape[0]} individuals and {empirical_array.shape[1]} SNPs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choose values for down-projection\n",
    "\n",
    "SFS cannot be generated from datasets that include missing data. To circumvent this, we use a downsampling approach such as that described in Satler and Carstens [(2017)](https://doi.org/10.1111/mec.14137). We must choose thresholds for each populations (i.e., the minumum number of individuals that must be sampled for a SNP to be used.) To help with this, we use the function find_downsampling from the class DataProcessor. This function generates a dictionary that holds the number of SNPs that meet each threshold.\n",
    "\n",
    "We will use a folded SFS, meaning that we will build the SFS based on minor allele frequencies.\n",
    "\n",
    "Since our data should be phased, and we will simulate diploid individuals, we will only consider multiples of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(8, 6, 6): 1034, (8, 6, 4): 1037, (8, 6, 2): 1037, (8, 4, 6): 1034, (8, 4, 4): 1037, (8, 4, 2): 1037, (8, 2, 6): 1034, (8, 2, 4): 1037, (8, 2, 2): 1037, (6, 6, 6): 1035, (6, 6, 4): 1038, (6, 6, 2): 1038, (6, 4, 6): 1035, (6, 4, 4): 1038, (6, 4, 2): 1038, (6, 2, 6): 1035, (6, 2, 4): 1038, (6, 2, 2): 1038, (4, 6, 6): 1035, (4, 6, 4): 1038, (4, 6, 2): 1038, (4, 4, 6): 1035, (4, 4, 4): 1038, (4, 4, 2): 1038, (4, 2, 6): 1035, (4, 2, 4): 1038, (4, 2, 2): 1038, (2, 6, 6): 1035, (2, 6, 4): 1038, (2, 6, 2): 1038, (2, 4, 6): 1035, (2, 4, 4): 1038, (2, 4, 2): 1038, (2, 2, 6): 1035, (2, 2, 4): 1038, (2, 2, 2): 1038}\n"
     ]
    }
   ],
   "source": [
    "# generate dictionary with the number of SNPs at different sampling thresholds\n",
    "empirical_2d_sfs_sampling = data_processor.find_downsampling(empirical_array)\n",
    "\n",
    "# print threshold, SNP combos for thresholds with at least 1030 snps\n",
    "minspns = 1030\n",
    "min_filtered = {key: value for key, value in empirical_2d_sfs_sampling.items() if value >= minspns}\n",
    "print(min_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build SFS with down-projection\n",
    "\n",
    "Based on the results above, we need to choose a threshold for down-sampling. We want to strike a balance between the number of SNPs and the number of individuals we keep in our final SFS. By keeping eight individuals from population A, six individuals from population B, and six individuals from population C, we can use data from 1034 SNPs.\n",
    "\n",
    "Now, we are ready to build our SFS. Note that our actual SFS will contain fewer than 1034 SNPs because some SNPs will become invariable when we downsample. Since we are downsampling, we will create ten replicate SFS.\n",
    "\n",
    "We will build a set of joint SFS (jSFS) between each pairwise combination of populations. Additionally, we will build a multidimensional SFS (mSFS). When building the mSFS, we can optionally choose to use a binning approach [(Smith et al., 2018)](https://doi.org/10.1111/mec.14223). This creates a coarser SFS, which can be helpful if we have a lot of individuals and not that many SNPs.\n",
    "\n",
    "The code below will also print the number of SNPs used on average to construct the mSFS, which will be helpful information when deciding how many SNPs to use to build the SFS for the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We used an average of 333.4 to construct the mSFS.\n"
     ]
    }
   ],
   "source": [
    "downsampling_dictionary = {\"A\":8, \"B\":6, \"C\":6}\n",
    "\n",
    "# build 10 replicates of the 2d SFS\n",
    "empirical_2d_sfs = data_processor.numpy_to_2d_sfs(empirical_array, downsampling=downsampling_dictionary, replicates = 10)\n",
    "\n",
    "# build 10 replicates of the mSFS\n",
    "empirical_msfs, average_snps = data_processor.numpy_to_msfs(empirical_array, downsampling=downsampling_dictionary, replicates = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the models, and draw parameters from priors\n",
    "\n",
    "Next, we will use functions from the ModelBuilder class to generate a baseline set of models (without parameters). Then, we use draw_parameters to draw parameterized models from the parameter space. This will return a lists of lists. Each list will correspond to a model. For each model, we have a list of parameterized versions of that model, with parameters drawn from the priors defined by the user in the configuration and species tree files.\n",
    "\n",
    "A full description of the models generated by delimitpy is available [here](https://delimitpy.readthedocs.io/en/latest/usage/buildingmodels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# build the models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model_builder \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_models\u001b[49m\u001b[38;5;241m.\u001b[39mModelBuilder(config_values)\n\u001b[1;32m      3\u001b[0m model_builder\u001b[38;5;241m.\u001b[39mbuild_models()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# parameterize the models\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_models' is not defined"
     ]
    }
   ],
   "source": [
    "# build the models\n",
    "model_builder = generate_models.ModelBuilder(config_values)\n",
    "model_builder.build_models()\n",
    "\n",
    "# parameterize the models\n",
    "parameterized_models, labels  = model_builder.draw_parameters()\n",
    "\n",
    "print((f\"Number of models: {len(parameterized_models)}\"))\n",
    "print((f\"Number of replicates: {len(parameterized_models[0])}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
