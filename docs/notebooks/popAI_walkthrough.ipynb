{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 09:25:44.321303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from popai import parse_input\n",
    "from popai import process_empirical\n",
    "from popai import generate_models\n",
    "from popai import simulate_data\n",
    "from popai import build_predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# popai walk-through\n",
    "\n",
    "This jupyter notebook walks through each step in running popai, including parsing and reading input data, building SFS, building models, simulating data, training machine learning models, and applying models to empirical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parse the configuration file\n",
    "\n",
    "We will use the ModelConfigParser class to parse the information from the configuration. A full description of the configuration file, along with an example, is available [here](https://popai.readthedocs.io/en/latest/usage/parsinginput.html). This file provides popai with the information it needs to conduct a species delimitation analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_parser = parse_input.ModelConfigParser(\"../input_files/config.txt\")\n",
    "config_values = config_parser.parse_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Process empirical data\n",
    "\n",
    "Next, we will process our empirical data. The format for the empirical data is described in detail [here](https://delimitpy.readthedocs.io/en/latest/usage/parsinginput.html). \n",
    "\n",
    "There are three major steps:\n",
    "1) Read data from fasta files and convert to a numpy array.\n",
    "2) Choose values for down-projecting the Site Frequency Spectrum.\n",
    "3) Build SFS with down-projection.\n",
    "4) Determine the average number of SNPs used to construct the SFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a: Read data into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = process_empirical.DataProcessor(config=config_values)\n",
    "empirical_array = data_processor.fasta_to_numpy()\n",
    "print(f\"Our input data has {empirical_array.shape[0]} individuals and {empirical_array.shape[1]} SNPs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Choose values for down-projection\n",
    "\n",
    "SFS cannot be generated from datasets that include missing data. To circumvent this, we use a downsampling approach such as that described in Satler and Carstens [(2017)](https://doi.org/10.1111/mec.14137). We must choose thresholds for each populations (i.e., the minumum number of individuals that must be sampled for a SNP to be used.) To help with this, we use the function find_downsampling from the class DataProcessor. This function generates a dictionary that holds the number of SNPs that meet each threshold.\n",
    "\n",
    "We will use a folded SFS, meaning that we will build the SFS based on minor allele frequencies.\n",
    "\n",
    "Since our data should be phased, and we will simulate diploid individuals, we will only consider multiples of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dictionary with the number of SNPs at different sampling thresholds\n",
    "empirical_2d_sfs_sampling = data_processor.find_downsampling(empirical_array)\n",
    "\n",
    "# print threshold, SNP combos for thresholds with at least 1030 snps\n",
    "minspns = 1030\n",
    "min_filtered = {key: value for key, value in empirical_2d_sfs_sampling.items() if value >= minspns}\n",
    "print(min_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2c: Build SFS with down-projection\n",
    "\n",
    "We want to maximize the number of individuals and SNPs we can use. In this tutorial, since the input data were simulated without missing data, we can use all individuals and still retain all the SNPs.\n",
    "\n",
    "Now, we are ready to build our SFS. We are only using a single replicate for this test. This makes sense because our ‘empirical’ data are actually simulated data, and we are not downsampling. Because of this, we do not expect much noise. For messier empirical data, use ~10 reps and ensure that results do not differ across replicates.\n",
    "\n",
    "We will build a set of joint SFS (jSFS) between each pairwise combination of populations. Additionally, we will build a multidimensional SFS (mSFS). When building the mSFS, we can optionally choose to use a binning approach [(Smith et al., 2018)](https://doi.org/10.1111/mec.14223). This creates a coarser SFS, which can be helpful if we have a lot of individuals and not that many SNPs.\n",
    "\n",
    "The code below will also print the number of SNPs used on average to construct the mSFS, which will be helpful information when deciding how many SNPs to use to build the SFS for the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampling_dictionary = {\"A\":20, \"B\":20, \"C\":20}\n",
    "\n",
    "# build 10 replicates of the 2d SFS\n",
    "empirical_2d_sfs = data_processor.numpy_to_2d_sfs(empirical_array, downsampling=downsampling_dictionary, replicates = 1)\n",
    "\n",
    "# build 10 replicates of the mSFS\n",
    "empirical_msfs, average_snps = data_processor.numpy_to_msfs(empirical_array, downsampling=downsampling_dictionary, replicates = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the models, and draw parameters from priors\n",
    "\n",
    "Next, we will use functions from the ModelBuilder class to generate a baseline set of models (without parameters). Then, we use draw_parameters to draw parameterized models from the parameter space. This will return a lists of lists. Each list will correspond to a model. For each model, we have a list of parameterized versions of that model, with parameters drawn from the priors defined by the user in the configuration and species tree files.\n",
    "\n",
    "A full description of the models generated by popai is available [here](https://popai.readthedocs.io/en/latest/usage/buildingmodels.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the models\n",
    "model_builder = generate_models.ModelBuilder(config_values)\n",
    "divergence_demographies, sc_demographies, dwg_demographies = model_builder.build_models()\n",
    "\n",
    "# parameterize the models\n",
    "parameterized_models, labels = model_builder.draw_parameters(divergence_demographies, sc_demographies, dwg_demographies)\n",
    "\n",
    "print((f\"Number of models: {len(parameterized_models)}\"))\n",
    "print((f\"Number of replicates: {len(parameterized_models[0])}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate the models by  plotting\n",
    "\n",
    "Now, we must ask \"Are these the models I'm looking for?\"\n",
    "\n",
    "One way to easily check this is by plotting the models. The validate_models function takes as input your lists returned from the parameterization command above (draw_parameters). For each model, it will draw one parameterization at random, and plot it using functionality from the drawdemes package.\n",
    "\n",
    "Make sure the models look like you hoped they would! Remember, you are only seeing a single parameterization for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder.validate_models(parameterized_models, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simulate Data\n",
    "\n",
    "Now that we have processed our empirical data and build our models, we are ready to simulate the data that we will use to train our machine learning algorithms. We will use information from processing our empirical data (the number of individuals and SNPs to use when bulding the SFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simulator = simulate_data.DataSimulator(parameterized_models, labels, config=config_values, cores=1, downsampling=downsampling_dictionary, max_sites = average_snps)\n",
    "\n",
    "arrays, labels = data_simulator.simulate_ancestry() # simulate ancestry in msprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build SFS from simulated data\n",
    "\n",
    "Now, we will create a numpy array and build jSFS and mSFS from our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mSFS = data_simulator.mutations_to_sfs(arrays) # generate mSFS\n",
    "jSFS = data_simulator.mutations_to_2d_sfs(arrays) # generate 2D SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Classifiers\n",
    "\n",
    "We now have our simulated training data and are ready to train our classifiers. We will train three classifiers.\n",
    "\n",
    "1) A Random Forest classifier that takes as input the bins of the multidimensional SFS (mSFS).\n",
    "\n",
    "2) A Fully Connected Neural Network that takes as input the bins of the multidimensional SFS (mSFS).\n",
    "\n",
    "3) A Convolutional Neural Network that takes as input the jSFS between all pairs of populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7a: Train the RF classifier\n",
    "\n",
    "The code will return the model and the confusion matrix and will print the out-of-bag error rates and the confusion matrix on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_sfs_predictor = build_predictors.RandomForestsSFS(config_values, mSFS, labels)\n",
    "random_forest_sfs_model, random_forest_sfs_cm, random_forest_sfs_cm_plot = random_forest_sfs_predictor.build_rf_sfs()\n",
    "random_forest_sfs_cm_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7b: Train the FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_sfs_predictor = build_predictors.NeuralNetSFS(config_values, mSFS, labels)\n",
    "neural_network_sfs_model, neural_network_sfs_cm, neural_network_sfs_cm_plot = neural_network_sfs_predictor.build_neuralnet_sfs()\n",
    "neural_network_sfs_cm_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7c: Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_2d_sfs_predictor = build_predictors.CnnSFS(config_values, jSFS, labels)\n",
    "cnn_2d_sfs_model, cnn_2d_sfs_cm, cnn_2d_sfs_cm_plot = cnn_2d_sfs_predictor.build_cnn_sfs()\n",
    "cnn_2d_sfs_cm_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Now, we are ready to apply our classifiers to our empirical data.\n",
    "\n",
    "We will use the three classifiers trained above to make predictions on our empirical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8a: Random Forest predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = random_forest_sfs_predictor.predict(random_forest_sfs_model, empirical_msfs)\n",
    "print(results_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8b: FCNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fcnn = neural_network_sfs_predictor.predict(neural_network_sfs_model, empirical_msfs)\n",
    "print(results_fcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8c: CNN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cnn = cnn_2d_sfs_predictor.predict(cnn_2d_sfs_model, empirical_2d_sfs)\n",
    "print(results_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
